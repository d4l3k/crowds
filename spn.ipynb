{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T05:51:39.488958Z",
     "start_time": "2019-07-13T05:51:33.098754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffda351e88a4263866c8e59dc656abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='_get_local_bins', max=6604, style=ProgressStyle(description_wâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cc8e6a018440>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m train_set = IOGDataset(val=False, transform=[\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomGrayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m ])\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-cc8e6a018440>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, val, transform)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIOGDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'local_bins'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mIOGDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_local_bins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_bin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-cc8e6a018440>\u001b[0m in \u001b[0;36m_get_local_bins\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeople\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"_get_local_bins\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeople\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-cc8e6a018440>\u001b[0m in \u001b[0;36m_get_density\u001b[0;34m(self, people, i, j, h, w)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         ).squeeze(dim=0)\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import random\n",
    "from torchvision.models.vgg import vgg16_bn\n",
    "import math\n",
    "\n",
    "num_workers = 16\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "Person = collections.namedtuple(\"Person\", [\"leftx\", 'lefty', 'rightx', 'righty', 'age', 'gender'])\n",
    "\n",
    "def iog_files():\n",
    "    files = []\n",
    "    for file in glob.glob(\"iog_dataset/*/PersonData.txt\"):\n",
    "        dirname = os.path.dirname(file)\n",
    "        with open(file) as f:\n",
    "            pic = None\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()\n",
    "                if line.endswith(\".jpg\"):\n",
    "                    if pic is not None:\n",
    "                        src = os.path.join(dirname, pic)\n",
    "                        files.append((src, poses))\n",
    "\n",
    "                    pic = line\n",
    "                    poses = []\n",
    "                else:\n",
    "                    p = Person(*map(int, line.split(\"\\t\")))\n",
    "                    poses.append(((p.leftx + p.rightx)/2, (p.lefty+p.righty)/2))\n",
    "    return files\n",
    "assert len(iog_files())\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def qrnf_files():\n",
    "    files = []\n",
    "    for file in glob.glob('UCF-QNRF_ECCV18/*/*.jpg'):\n",
    "        basepath = os.path.splitext(file)[0]\n",
    "        metapath = basepath + \"_ann.mat\"\n",
    "        meta = loadmat(metapath)\n",
    "        files.append((file, meta['annPoints']))\n",
    "    return files\n",
    "\n",
    "assert len(qrnf_files())\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "class IOGDataset(torch.utils.data.dataset.Dataset):\n",
    "    def __init__(self, val, transform=[]):\n",
    "        self.files = iog_files() + qrnf_files()\n",
    "        #self.files = qrnf_files()\n",
    "        self.val = val\n",
    "        self.transforms = transforms.Compose(transform + [\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        self.scale = (0.08, 1.0)\n",
    "        self.ratio = (3. / 4., 4. / 3.)\n",
    "        self.flip_chance = 0.5\n",
    "        self.size = 64\n",
    "        self.density_size = self.size\n",
    "        self.interpolation = Image.BILINEAR\n",
    "        sizes = [len(people) for _, people in self.files]\n",
    "        self.bins = np.percentile(sizes, (33.3, 66.6, 100))\n",
    "        self.local_window = torch.ones((1, 1, 31, 31))\n",
    "        if getattr(IOGDataset, 'local_bins', None) is None:\n",
    "            IOGDataset.local_bins = self._get_local_bins()\n",
    "        \n",
    "    def _get_bin(self, people):\n",
    "        for i, b in enumerate(self.bins):\n",
    "            if people <= b:\n",
    "                return i\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def _get_local_bins(self):\n",
    "        sets = []\n",
    "        for path, people in tqdm(self.files, desc=\"_get_local_bins\"):\n",
    "            size = Image.open(path).size\n",
    "            _, _, local = self._get_density(people, 0, 0, size[1], size[0])\n",
    "            for n in local.numpy().flatten():\n",
    "                if n > 0:\n",
    "                    sets.append(n)\n",
    "        return np.percentile(sets, (33.3, 66.6, 100))\n",
    "    \n",
    "    def _get_density(self, people, i, j, h, w):\n",
    "        density = torch.zeros((self.density_size, self.density_size))\n",
    "        in_window = 0\n",
    "        for (x, y) in people:\n",
    "            newx = int((x-j)/w*self.density_size)\n",
    "            newy = int((y-i)/h*self.density_size)\n",
    "            if newx < 0 or newx >= self.density_size or newy < 0 or newy >= self.density_size:\n",
    "                continue\n",
    "            density[newy, newx] += 1.0\n",
    "            in_window += 1\n",
    "            \n",
    "        density = density.unsqueeze(dim=0)\n",
    "        \n",
    "        local = nn.functional.conv2d(\n",
    "            density.unsqueeze(dim=0), \n",
    "            self.local_window, \n",
    "            padding=int(self.local_window.shape[-1]/2),\n",
    "            stride=4,\n",
    "        ).squeeze(dim=0)\n",
    "            \n",
    "        return density, in_window, local\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, people = self.files[index]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.val:\n",
    "            i = 0\n",
    "            j = 0\n",
    "            h = img.height\n",
    "            w = img.width\n",
    "        else:\n",
    "            i, j, h, w = transforms.RandomResizedCrop.get_params(img, self.scale, self.ratio)\n",
    "            \n",
    "        img = transforms.functional.resized_crop(img, i, j, h, w, (self.size, self.size), self.interpolation)\n",
    "        \n",
    "        density, in_window, local = self._get_density(people, i, j, h, w)\n",
    "        local = local.squeeze(dim=0)\n",
    "        \n",
    "        local_size = int(self.density_size / 4)\n",
    "        local_classes = torch.zeros((local_size, local_size), dtype=torch.uint8)\n",
    "        for p in IOGDataset.local_bins:\n",
    "            matches = local > p\n",
    "            local_classes += matches\n",
    "            \n",
    "        global_class = self._get_bin(in_window)\n",
    "        \n",
    "        img = self.transforms(img)\n",
    "        if not self.val and random.random() < self.flip_chance:\n",
    "            img = img.flip(dims=(2,))\n",
    "            density = density.flip(dims=(2,))\n",
    "        return img, density, global_class, local_classes.long()\n",
    "\n",
    "train_set = IOGDataset(val=False, transform=[\n",
    "    transforms.RandomGrayscale()\n",
    "])\n",
    "\n",
    "img, density, _, local_classes = train_set[0]\n",
    "print(local_classes, img.shape, density.shape)\n",
    "display(transforms.functional.to_pil_image(img), transforms.functional.to_pil_image(density))\n",
    "\n",
    "val_set = IOGDataset(val=True)\n",
    "\n",
    "indicies = np.random.permutation(len(train_set))\n",
    "n_training_samples = int(len(train_set) * 0.95)\n",
    "train_sampler = SubsetRandomSampler(indicies[:n_training_samples])\n",
    "val_sampler = SubsetRandomSampler(indicies[n_training_samples:])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers=num_workers,\n",
    "                                           drop_last=True, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                            sampler=val_sampler, num_workers=num_workers,\n",
    "                                        pin_memory=True)\n",
    "\n",
    "assert len(val_loader)\n",
    "assert len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiScaleAnalysis(nn.Module):\n",
    "    def __init__(self, k, out):\n",
    "        super().__init__()\n",
    "        per = int(out/4)\n",
    "        self.stack1 = nn.Conv2d(k, per, 1)\n",
    "        self.stack2 = nn.Sequential(\n",
    "            nn.Conv2d(k, 32, 1),\n",
    "            nn.Conv2d(32, per, 3, padding=1),\n",
    "        )\n",
    "        self.stack3 = nn.Sequential(\n",
    "            nn.Conv2d(k, 32, 1),\n",
    "            nn.Conv2d(32, per, 5, padding=2),\n",
    "        )\n",
    "        self.stack4 = nn.Sequential(\n",
    "            nn.Conv2d(k, 32, 1),\n",
    "            nn.Conv2d(32, per, 7, padding=3),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([\n",
    "            self.stack1(x),\n",
    "            self.stack2(x),\n",
    "            self.stack3(x),\n",
    "            self.stack4(x)\n",
    "        ], dim=1)\n",
    "\n",
    "class InceptionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        \n",
    "        k = 64\n",
    "        self.conv1 = MultiScaleAnalysis(3, k)\n",
    "        self.conv2 = MultiScaleAnalysis(k, k)\n",
    "        self.conv3 = MultiScaleAnalysis(k, k)\n",
    "        self.conv4 = MultiScaleAnalysis(k, k)\n",
    "        self.conv5 = MultiScaleAnalysis(k, k)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(k, 1, 3)\n",
    "\n",
    "    def forward(self, orig):\n",
    "        x = self.relu(self.conv1(orig))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.relu(self.conv5(x))\n",
    "        x = self.conv6(x)\n",
    "        return x\n",
    "    \n",
    "padding_type = 'reflect'\n",
    "norm_layer = nn.BatchNorm2d\n",
    "use_dropout = False\n",
    "use_bias = False\n",
    "    \n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"Initialize the Resnet block\n",
    "        A resnet block is a conv block with skip connections\n",
    "        We construct a conv block with build_conv_block function,\n",
    "        and implement skip connections in <forward> function.\n",
    "        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "        \"\"\"\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Construct a convolutional block.\n",
    "        Parameters:\n",
    "            dim (int)           -- the number of channels in the conv layer.\n",
    "            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "            use_bias (bool)     -- if the conv layer uses bias or not\n",
    "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "        \"\"\"\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), \n",
    "            norm_layer(dim), \n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), \n",
    "            norm_layer(dim)\n",
    "        ]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function (with skip connections)\"\"\"\n",
    "        out = x + self.conv_block(x)  # add skip connections\n",
    "        return out\n",
    "    \n",
    "class LocalAttention(nn.Module):\n",
    "    def __init__(self, h, w):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 1, 31, padding=15, bias=use_bias),\n",
    "            norm_layer(1),\n",
    "            relu(),\n",
    "            \n",
    "            nn.Conv2d(1, 8, 5, padding=2, bias=use_bias),\n",
    "            norm_layer(8),\n",
    "            relu(),\n",
    "            \n",
    "            nn.Conv2d(8, 64, 3, padding=1, bias=use_bias),\n",
    "            norm_layer(64),\n",
    "            relu(),\n",
    "            \n",
    "            pool(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1, bias=use_bias),\n",
    "            norm_layer(128),\n",
    "            relu(),\n",
    "            \n",
    "            ResnetBlock(128),\n",
    "            \n",
    "            pool(),\n",
    "            \n",
    "            ResnetBlock(128),\n",
    "        )\n",
    "        self.h = int(h / 4)\n",
    "        self.w = int(w / 4)\n",
    "        self.fc_size = 128 * self.h * self.w\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.fc_size, 1024),\n",
    "            relu(),\n",
    "            nn.Linear(1024, 512),\n",
    "            relu(),\n",
    "            nn.Linear(512, 3 * self.h * self.w),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, self.fc_size)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 3, self.h, self.w)\n",
    "        return x\n",
    "       \n",
    "    \n",
    "def relu():\n",
    "    return nn.LeakyReLU(0.1, inplace=True)\n",
    "\n",
    "def pool():\n",
    "    return nn.MaxPool2d(2, 2)\n",
    "\n",
    "def global_attention():\n",
    "    vgg = vgg16_bn(pretrained=True)\n",
    "    for param in vgg.features.parameters():\n",
    "        param.require_grad = False\n",
    "    vgg.classifier = nn.Sequential(\n",
    "        nn.Linear(25088, 512),\n",
    "        relu(),\n",
    "        nn.Linear(512, 256),\n",
    "        relu(),\n",
    "        nn.Linear(256, 3),\n",
    "        nn.Softmax(),\n",
    "    )\n",
    "    return vgg\n",
    "\n",
    "class SPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        h = train_set.size\n",
    "        w = train_set.size\n",
    "        \n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 11, padding=5, bias=use_bias),\n",
    "            norm_layer(16),\n",
    "            relu(),\n",
    "            \n",
    "            pool(),\n",
    "            \n",
    "            nn.Conv2d(16, 24, 9, padding=4, bias=use_bias),\n",
    "            norm_layer(24),\n",
    "            relu(),\n",
    "            \n",
    "            nn.Conv2d(24, 16, 7, padding=3, bias=use_bias),\n",
    "            norm_layer(16),\n",
    "            relu(),\n",
    "            \n",
    "            pool(),\n",
    "            \n",
    "            nn.Conv2d(16, 16, 7, padding=3, bias=use_bias),\n",
    "            norm_layer(16),\n",
    "            relu(),\n",
    "            \n",
    "            nn.Conv2d(16, 8, 5, padding=2, bias=use_bias),\n",
    "            norm_layer(8),\n",
    "            relu(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 9, padding=4, bias=use_bias),\n",
    "            norm_layer(16),\n",
    "            relu(),\n",
    "            \n",
    "            pool(),\n",
    "            \n",
    "            nn.Conv2d(16, 24, 7, padding=3, bias=use_bias),\n",
    "            norm_layer(24),\n",
    "            relu(),\n",
    "            \n",
    "            nn.Conv2d(24, 32, 5, padding=2, bias=use_bias),\n",
    "            norm_layer(32),\n",
    "            relu(),\n",
    "            \n",
    "            pool(),\n",
    "            \n",
    "            nn.Conv2d(32, 32, 5, padding=2, bias=use_bias),\n",
    "            norm_layer(32),\n",
    "            relu(),\n",
    "            nn.Conv2d(32, 16, 3, padding=1, bias=use_bias),\n",
    "            norm_layer(16),\n",
    "            relu(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 7, padding=3, bias=use_bias),\n",
    "            norm_layer(16),\n",
    "            relu(),\n",
    "            \n",
    "            pool(),\n",
    "            \n",
    "            nn.Conv2d(16, 24, 5, padding=2, bias=use_bias),\n",
    "            norm_layer(24),\n",
    "            relu(),\n",
    "            \n",
    "            nn.Conv2d(24, 48, 3, padding=1, bias=use_bias),\n",
    "            norm_layer(48),\n",
    "            relu(),\n",
    "            \n",
    "            pool(),\n",
    "            \n",
    "            nn.Conv2d(48, 48, 3, padding=1, bias=use_bias),\n",
    "            norm_layer(48),\n",
    "            relu(),\n",
    "            \n",
    "            nn.Conv2d(48, 24, 3, padding=1, bias=use_bias),\n",
    "            norm_layer(24),\n",
    "            relu(),\n",
    "        )\n",
    "        self.global_attention = global_attention()\n",
    "        self.local_attention = LocalAttention(h, w)\n",
    "        self.fusion_network = nn.Sequential(\n",
    "            nn.Conv2d(48, 128, 3, padding=1, bias=use_bias),\n",
    "            norm_layer(128),\n",
    "            relu(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1, bias=use_bias),\n",
    "            norm_layer(128),\n",
    "            relu(),\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1, bias=use_bias),\n",
    "            norm_layer(64),\n",
    "            relu(),\n",
    "            nn.ConvTranspose2d(64, 16, 3, stride=2, padding=1, output_padding=1, bias=use_bias),\n",
    "            norm_layer(16),\n",
    "            relu(),\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(16, 1, 7),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        g = self.global_attention(x)\n",
    "        g_squeezed = g.view(-1, 3, 1, 1, 1)\n",
    "        \n",
    "        g1 = g_squeezed[:, 0]\n",
    "        g2 = g_squeezed[:, 1]\n",
    "        g3 = g_squeezed[:, 2]\n",
    "        \n",
    "        f1 = self.cnn1(x)\n",
    "        f2 = self.cnn2(x)\n",
    "        f3 = self.cnn3(x)\n",
    "        \n",
    "        l = self.local_attention(x)\n",
    "        l1 = l[:, 0].unsqueeze(dim=1)\n",
    "        l2 = l[:, 1].unsqueeze(dim=1)\n",
    "        l3 = l[:, 2].unsqueeze(dim=1)\n",
    "        \n",
    "        a1 = g1 * l1 * f1\n",
    "        a2 = g2 * l2 * f2\n",
    "        a3 = g3 * l3 * f3\n",
    "        \n",
    "        out = torch.cat((a1, a2, a3), dim=1)\n",
    "        out = self.fusion_network(out)\n",
    "        return out, g, l\n",
    "\n",
    "net = SPN()\n",
    "print(net)\n",
    "net.to(device)\n",
    "\n",
    "net(next(train_loader.__iter__())[0].to(device))\n",
    "\n",
    "density_criterion = nn.MSELoss()\n",
    "global_scale_criterion = nn.CrossEntropyLoss()\n",
    "local_scale_criterion = nn.CrossEntropyLoss()\n",
    "gs_lambda = 0.05\n",
    "ls_lambda = 0.05\n",
    "\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.0001, weight_decay=1e-6)\n",
    "#optimizer = optim.RMSprop(net.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "def compute_accuracy(a, b):\n",
    "    size = train_set.density_size\n",
    "    return (\n",
    "        a.view(-1, size*size).sum(1).round().int() == b.view(-1, size*size).sum(1).round().int()\n",
    "    ).sum().item()\n",
    "\n",
    "def compute_err(a, b):\n",
    "    size = train_set.density_size\n",
    "    return (\n",
    "        a.view(-1, size*size).sum(1)- b.view(-1, size*size).sum(1)\n",
    "    ).abs().sum().item()\n",
    "\n",
    "def display_density(tensor, scale=1):\n",
    "    display(\n",
    "        transforms.functional.to_pil_image(\n",
    "            F.interpolate(\n",
    "                tensor.unsqueeze(dim=0), \n",
    "                scale_factor=(scale, scale)\n",
    "            ).squeeze(dim=0).cpu()\n",
    "        )\n",
    "    )\n",
    "\n",
    "def display_pair(inputs, outputs, labels):\n",
    "    with torch.no_grad():\n",
    "        display_density(inputs[0], scale=1)\n",
    "        display_density(\n",
    "            (torch.cat((labels[0], outputs[0]), dim=1) * 0.5).clamp(0, 1), \n",
    "            scale=1\n",
    "        )\n",
    "        \n",
    "def train_epoch(name, epoch, loader, val=False):\n",
    "    running_loss = 0.0\n",
    "    running_density_loss = 0.0\n",
    "    running_gs_loss = 0.0\n",
    "    running_ls_loss = 0.0\n",
    "    running_error = 0.0\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "    for inputs, labels, gs, ls in tqdm(loader, desc=\"{} - epoch {}\".format(name, epoch)):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        gs = gs.to(device, non_blocking=True)\n",
    "        ls = ls.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, global_out, local_out = net(inputs)\n",
    "        density_loss = density_criterion(outputs, labels)\n",
    "        gs_loss = gs_lambda * global_scale_criterion(global_out, gs)\n",
    "        ls_loss = ls_lambda * local_scale_criterion(local_out, ls)\n",
    "        loss = density_loss + gs_loss + ls_loss\n",
    "        if not val:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            running_error += compute_err(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            running_density_loss += density_loss.item()\n",
    "            running_gs_loss += gs_loss.item()\n",
    "            running_ls_loss += ls_loss.item()\n",
    "            total += len(labels)\n",
    "            accuracy += compute_accuracy(outputs, labels)\n",
    "        \n",
    "    print('{} - loss: {:.5f} (density: {:.5f}, gs: {:.5f}, ls: {:.5f}), accuracy: {:.3f}, error: {:.3f}'.format(\n",
    "        name,\n",
    "        running_loss / total, running_density_loss / total, running_gs_loss / total, \n",
    "        running_ls_loss / total, accuracy / total, running_error / total))\n",
    "    print(gs[0], global_out[0])\n",
    "    #print(ls[0]) #, local_out[0])\n",
    "    display_pair(inputs, outputs, labels)\n",
    "\n",
    "def train(epoch):\n",
    "    train_epoch('train', epoch, train_loader)\n",
    "    lr_scheduler.step()\n",
    "    with torch.no_grad():\n",
    "        train_epoch('val', epoch, val_loader, val=True)\n",
    "\n",
    "for epoch in range(10000): \n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "for epoch in range(200): \n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.bins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
